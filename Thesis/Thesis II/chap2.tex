\chapter{Related Work}

There have been many works on finding cognate pairs in languages, but the word lists used so far have been very limited. The conventional approaches developed for the task of cognate identification are usually based on a combination of different string similarity measures between a pair of words as features for different classifiers like maximum-entropy, decision trees and SVMs \cite{inkpen2005automatic}\cite{bergsma2007alignment}. These include orthographic and phonetic similarity as discussed earlier. The objective can be finding pairs of cognates among two related languages, or finding groups of cognates among multiple languages.

\textbf{Orthographic features based classifier : } Hauer and Kondrak \cite{hauer2011clustering} use a number of basic word similarity measures as input to a SVM classifier for cognate prediction. They use features like common bigrams, longest common substring, word length difference etc. They also define binary language pair features that help to encode the degree of affinity between pairs of languages, ie. the likelihood of two languages sharing cognates. After the classification of word pairs as cognates or non-cognates, they perform clustering over all lexical items from different languages and the same meaning. The clustering quality is evaluated against the gold standard cognacy judgments.

\textbf{Gap-weighted common subsequences : } T. Rama \cite{rama2015automatic} uses a string kernel based approach for automatic cognate identification. He defines  a normalized vector for every word, based on the various constituent subsequences of the word. The different dimensions of the vector correspond to the different subsequences of various lengths that can be formed over the set of characters. The weights in the vector for each subsequence is weighted by the count and gaps in the subsequence as present inside that word. By defining a common subsequence vector between the word pairs and using that as input features to the linear classifier, it is shown that subsequence based features outperform word similarity measures.

\textbf{Partial cognacy in morphemes : } List et al. \cite{listusing} introduce the novel notion of partial cogacy between words which is defined using cognate clusters of the constituent morphemes rather than the entire words. They motivate that normal cognate identification models perform poorly on South-East Asian languages due to the presence of large number of compound words in these languages. They predict the partial cognacy judgements by defining sequence similarity networks over the constituent elements or morphemes of the word. These networks are weighted by employing the same string similarity features as used in \cite{hauer2011clustering} but over the morphemes. They further use algorithms for network partitioning to find clusters of cognate morphemes.

\textbf{Siamese ConvNet model : } In a recent work, T. Rama introduces Convolutional Neural Network based siamese-style model \citep{rama2016siamese} for the task. The model is inspired by image-similarity CNN models. Here each word is transcribed using phonetic characters and transformed using manually defined embedding vectors for the different phonetic characters. This leads to each word being treated as a 2D image comprising of a vector of phonetic character embeddings. By using a deep learning based model, the need for external feature engineering is avoided and the system outperforms previous works on the task.

In our work, instead of CNNs, we propose the use of LSTM networks \citep{lstm1997}. As LSTMs are a form of recurrent network (RNN), they fit more naturally to natural language, which also has a sequential architecture and follows a linear order. In comparison, convolutional networks are hierarchical and hence seem natural for images. Even though NLP literature does not support a clear distinction between the domains where CNNs or RNNs perform better, recent works have shown that each provide complementary information for text classification tasks \citep{yin2017comparative}. 
