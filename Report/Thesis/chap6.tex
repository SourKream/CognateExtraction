\chapter{Future Work}

Motivated by the results of our analysis over the existing models, we have seen that semantics of the words influence the behavior and performance of the models. All the previous works on cognate identification mainly use phonetic or orthographic features of words for this judgment. The subsequence model particularly considers only such features. However it is evident that the semantics are also important features that should be considered while predicting cognates. Also, since the aim of this project is to use the model on a larger vocabulary of data from the aligned corpus, where the words are not grouped by meanings as in word lists, the model is expected to get confused without any semantic information. We propose to introduce this semantic information by utilizing the word embedding features in out model. 

Word embeddings are representation features where the words in the vocabulary are represented as points in a low dimensional space as compared to the vocabulary size. These are learnt by using a unsupervised probabilistic modeling approach with deep learning models. They are task independent features that are arranged such that their structure captures some sort of semantic relationships between the words. It has been shown that word embedding vectors capture semantic information \cite{faruqui2014retrofitting} and there have also been works to improve these semantics encoded in the embedding \cite{maas2010probabilistic}. We propose to use the multilingual word embeddings Polyglot \cite{polyglot:2013:ACL-CoNLL} that provide word vector embeddings for 116 languages over a rich vocabulary. These are trained on the processed Wikipedia text dumps of the various languages. 

Along with using the word embedding features in our model, we would also like to move towards a neural network based model for classification of cognates. By utilizing recurrent networks like RNNs and LSTMs to encode the input words (character sequences) at the character level, we can use attention based models to classify the word pairs. Such a model will not be prone to problems like sparse vectors and loss of information in the case of the \textit{Multiplicative} model.

As mentioned earlier in our analysis, we would also like to try and test a Hybrid model  between the \textit{Additive} and the \textit{Multiplicative} models. Such a model can be implemented be considering the vector output by the \textit{Multiplicative} model, and then stealing and redistributing the weights from all the high positive feature to the zero subsequence features that are present in the string. This concept is inspired from smoothing of sparse vectors. Some initial analysis of such a model has provided interesting results that improve the performance of the \textit{Multiplicative} model, but we need to perform further analysis before reporting its results.

Once our models have been trained in the multilingual setting, we would like to apply it specifically to the domain of Hindi-Marathi using the sentence aligned corpus and evaluate the cognate pairs that we are able to discover. 

