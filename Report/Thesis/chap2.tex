\chapter{Previous Work}

There have been many works on find cognate pairs in languages, but the word lists used so far have been very limited. The conventional approaches developed for the task of cognate identification are usually based on a combination of different string similarity measures between a pair of words as features for different classifiers like maximum-entropy, decision trees and SVMs \cite{inkpen2005automatic}\cite{bergsma2007alignment}. These include orthographic and phonetic similarity. The objective can be finding pairs of cognates among two related languages, or finding groups of cognates among multiple languages.

Hauer and Kondrak \cite{hauer2011clustering} incorporate a number of diverse word similarity measures, that are manually identified, as input to a SVM classifier. These features can include number of shared bi-grams, edit distance, the length of strings and longest common subsequence. They also define binary language pair features that are believed to encode the degree of affinity between pairs of languages, ie. the likelihood of two languages sharing cognates or not. The authors employ these binary language-pair features that are used to weigh the language distance and assist the task of semantic clustering of cognates. After the classification of word pairs as cognates or non-cognates, they perform clustering over all lexical items from different languages and the same meaning. The clustering quality is evaluated against the gold standard cognacy judgments.

T. Rama \cite{rama2015automatic} uses a string kernel based approach for automatic cognate identification. He defines  a normalized vector for every word string, based on the various constituent subsequences of the word. The different dimensions dimensions of the vector correspond to the different subsequences of various lengths, that can be formed over the set of characters. The weights in the vector for each subsequence is weighted by the count and gaps in the subsequence inside that word. By defining a common subsequence vector between the word pairs and using that as input features to the linear classifier, he shows that subsequence based features outperform word similarity measures in this task.

List et al. \cite{listusing} introduce the novel notion of partial cogacy between words which is defined using cognate clusters of the constituent morphemes rather than the entire words. They motivate that normal cognate identification models perform poorly on South-East Asian languages due to the presence of large number of compound words in these languages. They predict the partial cognacy judgements by defining sequence similarity networks over the constituent elements or morphemes of the word. These networks are weighted by employing the same string similarity features as used in \cite{hauer2011clustering} but over the morphemes. They further use algorithms for network partitioning to find clusters of cognate morphemes.

In a recent work, T. Rama \cite{rama2016siamese} introduces a siamese convolutional network based approach for the cognate task, motivated by models used for image similarity tasks. Here each word is transcribed using phonetic characters and he manually defines vectors for the different phonetic alphabets. This  leads to each word being treated as a 2D image comprising of a vector of phonetic character vectors. These `images' can then be used in traditional siamese convolutional network models for image similarity for pairwise cognate identification.

