Shantanu@ ~/Documents/College/SemVIII/BTP/Code/RNNModel/ $ python SiameseModel.py -concept True
Couldn't import dot_parser, loading of dot files will not be possible.
Using Theano backend.
conv_dim 10
epochs 10
batch_size 128
xmaxlen 10
LR 0.001
Embedding Size 16
Tokenize Simple False
Using Concept Fold Data True
Vocab Size :  530
Building model
Word Length :  10
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
Input Word 1 (InputLayer)        (None, 10)            0                                            
____________________________________________________________________________________________________
Input Word 2 (InputLayer)        (None, 10)            0                                            
____________________________________________________________________________________________________
Embedding Layer (Embedding)      (None, 10, 16)        8480        Input Word 1[0][0]               
                                                                   Input Word 2[0][0]               
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 10, 16)        0           Embedding Layer[0][0]            
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 10, 16)        0           Embedding Layer[1][0]            
____________________________________________________________________________________________________
reshape_1 (Reshape)              (None, 10, 16, 1)     0           dropout_1[0][0]                  
____________________________________________________________________________________________________
reshape_2 (Reshape)              (None, 10, 16, 1)     0           dropout_2[0][0]                  
____________________________________________________________________________________________________
convolution2d_1 (Convolution2D)  (None, 8, 15, 10)     70          reshape_1[0][0]                  
                                                                   reshape_2[0][0]                  
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 6, 14, 10)     0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
maxpooling2d_2 (MaxPooling2D)    (None, 6, 14, 10)     0           convolution2d_1[1][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 840)           0           maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
flatten_2 (Flatten)              (None, 840)           0           maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
merge_1 (Merge)                  (None, 840)           0           flatten_1[0][0]                  
                                                                   flatten_2[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 1)             841         merge_1[0][0]                    
====================================================================================================
Total params: 9,391
Trainable params: 9,391
Non-trainable params: 0
____________________________________________________________________________________________________
Model Compiled
Training New Model
Epoch 1/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.4122 - precision: 0.6204 - recall: 0.3571 - fmeasure: 0.4242 

Training -> Precision:  0.880357931949 	 Recall:  0.468496009642 	 F-Score:  0.611547574511
Testing  -> Precision:  0.817226317226 	 Recall:  0.385214258139 	 F-Score:  0.523613783453 

258976/258976 [==============================] - 52s - loss: 0.4122 - precision: 0.6204 - recall: 0.3571 - fmeasure: 0.4242    
Epoch 2/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.3095 - precision: 0.8005 - recall: 0.6765 - fmeasure: 0.7296 

Training -> Precision:  0.856558801284 	 Recall:  0.615852672732 	 F-Score:  0.71653072043
Testing  -> Precision:  0.744403348258 	 Recall:  0.492085960623 	 F-Score:  0.592500774713 

258976/258976 [==============================] - 49s - loss: 0.3095 - precision: 0.8005 - recall: 0.6765 - fmeasure: 0.7296    
Epoch 3/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2850 - precision: 0.8133 - recall: 0.7179 - fmeasure: 0.7593 

Training -> Precision:  0.836716635436 	 Recall:  0.708784667617 	 F-Score:  0.767455739009
Testing  -> Precision:  0.70607472815 	 Recall:  0.555655642774 	 F-Score:  0.621898966622 

258976/258976 [==============================] - 50s - loss: 0.2850 - precision: 0.8133 - recall: 0.7178 - fmeasure: 0.7593    
Epoch 4/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2702 - precision: 0.8214 - recall: 0.7370 - fmeasure: 0.7739 

Training -> Precision:  0.847877656084 	 Recall:  0.740707535384 	 F-Score:  0.790677607117
Testing  -> Precision:  0.700237906423 	 Recall:  0.568137948784 	 F-Score:  0.627308894572 

258976/258976 [==============================] - 50s - loss: 0.2702 - precision: 0.8214 - recall: 0.7370 - fmeasure: 0.7739    
Epoch 5/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2609 - precision: 0.8252 - recall: 0.7529 - fmeasure: 0.7844 

Training -> Precision:  0.868145188827 	 Recall:  0.747350784109 	 F-Score:  0.803231946671
Testing  -> Precision:  0.726144489192 	 Recall:  0.568459657702 	 F-Score:  0.637698942582 

258976/258976 [==============================] - 49s - loss: 0.2609 - precision: 0.8252 - recall: 0.7529 - fmeasure: 0.7844    
Epoch 6/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2554 - precision: 0.8300 - recall: 0.7602 - fmeasure: 0.7905 

Training -> Precision:  0.825712944312 	 Recall:  0.801319831273 	 F-Score:  0.813333532237
Testing  -> Precision:  0.672059436189 	 Recall:  0.622764123021 	 F-Score:  0.646473417045 

258976/258976 [==============================] - 51s - loss: 0.2554 - precision: 0.8300 - recall: 0.7602 - fmeasure: 0.7905    
Epoch 7/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2519 - precision: 0.8322 - recall: 0.7637 - fmeasure: 0.7934 

Training -> Precision:  0.829677438325 	 Recall:  0.829421361278 	 F-Score:  0.829549380039
Testing  -> Precision:  0.664275658644 	 Recall:  0.637562733239 	 F-Score:  0.650645129518 

258976/258976 [==============================] - 50s - loss: 0.2519 - precision: 0.8322 - recall: 0.7637 - fmeasure: 0.7934    
Epoch 8/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2479 - precision: 0.8328 - recall: 0.7701 - fmeasure: 0.7974 

Training -> Precision:  0.815500763445 	 Recall:  0.847778479989 	 F-Score:  0.831326429874
Testing  -> Precision:  0.653024797603 	 Recall:  0.65911723073 	 F-Score:  0.656056870217 

258976/258976 [==============================] - 48s - loss: 0.2479 - precision: 0.8328 - recall: 0.7701 - fmeasure: 0.7974    
Epoch 9/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2441 - precision: 0.8336 - recall: 0.7758 - fmeasure: 0.8010 

Training -> Precision:  0.851831995589 	 Recall:  0.817340054968 	 F-Score:  0.834229652798
Testing  -> Precision:  0.687819420784 	 Recall:  0.623471882641 	 F-Score:  0.654066824165 

258976/258976 [==============================] - 49s - loss: 0.2441 - precision: 0.8336 - recall: 0.7758 - fmeasure: 0.8010    
Epoch 10/10
258944/258976 [============================>.] - ETA: 0s - loss: 0.2424 - precision: 0.8348 - recall: 0.7761 - fmeasure: 0.8016 

Training -> Precision:  0.834490301881 	 Recall:  0.841002954188 	 F-Score:  0.837733970675
Testing  -> Precision:  0.661477310368 	 Recall:  0.642452708789 	 F-Score:  0.651826223194 

258976/258976 [==============================] - 51s - loss: 0.2425 - precision: 0.8348 - recall: 0.7761 - fmeasure: 0.8016    
