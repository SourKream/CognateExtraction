
---------------------
6th Feb Morning:

 - Implemented word by word attention model (RNNModel) in Keras. Running following tests on HPC

   - Complex tokenisation (XMaxlen 10)
   - Simple tokenisation, Embedding size 60
   - Complex tokenisation, Regularisation 0.01
   - Complex tokenisation (XMaxlen 12)

  Playable parameters with model
    - Tokenisation
    - Embedding size
    - Regularisation

  Preliminary results:
   - DYEN Dataset (Would not be working with this dataset now. IPA domain more interesting)
     The initial model trained gave better performace that Subsequence model as shown in the PR Curves. 
     Models saved in HPC/Scratch/CognateModel/

   - IELex Dataset
     The simple tokenisation model was run for 2 epochs and the results wre significantly better than the subsequence model. There can be a possible 'logic error' in the implementation that the embedding size (200) was greater than the vocabulary size (161). Also complex tokenisation makes more sense phonetically so I tried that next. Still, this model has given the best PR Curve yet.
     The complex tokenisation model was tried again for 2 epochs on the local system but did not perform as good as simple tokenisation.

  Problems with existing model:
   - Keras implementation not proper
      - Running slow on HPC (???? What is this about ????)
      - Averaging of weights after every batch
      - Implement Theano Model for same

   - Keras Models in general were overfitting
      - After 3-4 epochs, there was large differce between training and testing
      - Testing Precision, Recall, FScore saturates
      - Regularisation? Unit-Norm for embeddings? Batch Normalisation?

   - Architecture problem
     - Non symmetric model
     - Try co-attention model

   - Error in 'Metrics' (RESOLVED)
     - The inbuild precision, recall, fscrore are not repoting correct figures
     - The defined precision recall fscore functions are not correct. Ambiguity of the input tensor to function.
     - The metrics reported by keras do not correspond with what is calculated in 'compute_acc'

  TODO:
   - Theano model
   - CoAttention model
   - Analysis of difference with tokenisation
   - Siamese model

---------------------
6th Feb Evening:

Updates
 - The 'error' in metrics issue is resolved. The system during training reported 'average' metric value across batches.
 - The output of the network has been changed from 2-dim with softmax to 1-dim with sigmoid. (Analyse affect on PR Curve for the same, sigmoid is a sharper function) and loss changed to binary_crossentropy from categorical_crossentropy.

---------------------
6th Feb Late Night:

- Keras model trained to maximum AUC of 0.83 on the val set for IELexDF1
- Implemented Theano code. Training is slow and not sure about the results currently. Update on that tomorrow morning.
- Increasing XMaxlen to 12 increased the Fscore by 2 points
- Need to try CoAttention Model
- Need to implement/ask for Siamese model
 
---------------------
8th Feb Meeting Notes:

Check performance on these sets,

- words with more cognates vs less cognates
- long words vs short words
- POS tag sets
- Divide datasets on meanings

- Correlation between pairwise distances, Spearman, Pearson, results unsatisfactory
- Models for learning Panphon using embeddings, doesnt seem feasible, varies results, very small dataset (233 chars)
- 

---------------------
1st March:

- Character Embedding Analysis
  - Pairwise distance correlation, no apparant relation
  - Pairwise distance correlation with different sets, no relation
  - Classifier to predict different classes, intractable
  - Diacritic vectors

- Error analysis test of best model
  - LanguageAnalysis.xlsx

- Siamese model performance
  - Error in implementation
  - Library version?

- Hindi-Marathi Domain
  - Devanagri to IPA tool
  - POS tagging available
  - NLTK lemmetizers
  - Linguistica Project (http://linguistica.uchicago.edu)

---------------------

 - Tests for rating word embeddings? Character embeddings?
 - Create dataset like Wordsim 353
   - Found low correlation for learnt embeddings. High for Panphon and Taraka embeddings

---------------------

 - Using Concept word vector as additional input feature like language feaures
 - Pretraing across language families
    - Results updated. Huge bonus for Mayan (Cross Language test)

---------------------

More properties on the datasets
chracter vocabs
Avergae word length
n-gram count
cognate counts in languages

- New concept features
- Pretraining not learning
  - Overlap of vocab b/w lang families
  - Overlap within dataset
- Error analysis
  - Concept Wise Performance
  - Language Wise Performance
- Hindi Marathi

- Transcription tests
- Character emebeddings
- Language annotated vocab?

---------------------

Number of unique words in each Lang family:
 - IELex    8622
 - Austro   10079
 - Mayan    1629

Number of words with more than 1 meaning:
 - IELex    667     ~07.7%  (1.11 meanings/word)
 - Austro   1119    ~11.1%  (1.17 meanings/word)
 - Mayan    148     ~09.1%  (1.11 meanings/word)

Number of words with more than 1 languages:
 - IELex    1333    ~15.4%  (1.3 langs/word)
 - Austro   1682    ~16.7%  (1.4 langs/word)
 - Mayan    384     ~23.6%  (1.7 langs/word))

Vocabulary Overlap between Language Families:
 - ielex and austro   558
 - ielex and mayan    141
 - mayan and austro   154

---------------------

TODO:

 - List of overlapping words                              DONE
 - IPA results                                            DONE
 - Hindi Marathi                                          DONE
 - Dataset stats                                          DONE
 - Pair counts for pair starting with (p,f), (p,v), etc
 - Error analysis exampples
 - Attention heat maps
 
---------------------

IPA vs ASJP

 - IPA does poorly on Guts, Vomit, Bad, Short, Thich as but ASJP does decent
 - IPA does well on Fire, Sleep, Pull, Smooth

 - For FIRE, ASJP gives many false positives
 - For SLEEP, ASJP gives false negatives 


---------------------

 - noun and verb list (50 each)

 - restrict to concepts
 - artificial samples
 
 - uniform attention test
 - taraka embeddings
 - performance falling on adding concepts?
 - Paper
 

---------------------

 - Models running last
    - IElex taraka embeddings     D
    - IElex + concept + taraka    D
    - CF IElex + taraka
    - Mayan + Taraka              D
    - Mayan + Concept + taraka    D
    - Austro + tarak              D 
    - Austro + Concept + taraka   D
    - IPA + IELEX + taraka             D
    - IPA + IELEX + Concept + taraka   D
    - Mayan + PretAustro + taraka   D
    - Mayan + PretIELEX + taraka    D
    - IELEX + PretAustro + taraka   D
    - Austro + PretIELEX + taraka   D
    - Mayan + Uniform               
    - Austro + Uniform

---------------------

  - Results on noun and verb lists
  - Uniform attention results
  - Taraka embeddings results


 - random embeddings vs init embeddings
 - rank dimensions to match spaces
 - normalise and cosine distance

 - motivation for co Attention      Done
 - Split related work               Done
 - footnote with taraka's results   Done
 - transcription tests to experiments sections
 - PR curves

 - related work on character level tasks
 - SCLeM website
 - fixed embeddings rather than initialisation onluy

 - rethink analysis sections
 - bring out properties of the datasets
 - bring back to problem in subseq model

---------------------

D - add word list table table 3.1 from thesis
D - problem definition in section 2
D - Match labels in image to section headings
D - First line section 3.2 repetitive  
D - section 3.3 attention explanaation. use example. compare against what subseq model is doing.
D - section 3.4 remove second para
D - section 3.4 refer back to the table 3.1 from thesis and relate. keep to 1 para
 - add line about non-linear classifier
D - remove ablation para. give a 'coming up'
D - dataset sectino comes before experiments
D - properties of datasets
D - table 3.2 from thesis

D - section 3.1: 
D    - Use '+ IE' consistently
D    - there are 2 ways of initialising the embeddings
D     - linguistic features of the phonemes
D     - random initialisations

D - experiment 1 : Indo European
D - experiment 2 : Pretraining (remove para about multi dataset experiments)
D - Table 3. Split experiments into addition of concept features and using pretraining.
D    - usefulness of CF comes from testing on more datasets
D    - pretrainign another exp. helps further
 
D - Counts tables can be brought together
D - Explain difference between cross concept and cross language

D - Move domain adaptation to experiments end
D  - talk about inflectional endings
  - add examples/images

 - Discussion section instead of analysis
    - Co Attention is effective at character level. Attention is very importnat without it its not working
    - Dataset size matters a lot. Particularly for small language families
    - Transcription doesnt really matter
    - Concept features help
    - Better performance on nouns and adjectives
    - Adaptable to text 

 - Embeddings analysis?
D - Transcription tests
 - Add highlights of paper at the end of intro

---------------------

 - Intro more catchy
D - Analysis on IELex only mention that
 - incremental pretraining
D - review
D - spell check


---------------------
POST TACL REVIEWS
---------------------

Improvements over current paper
  - visualtion of attention
  - using dummy examples to estimate learnt rules
  - punjabi as a 3rd test language along with hindi and marathi
  - test only known verbs in hindi-marathi?
  

