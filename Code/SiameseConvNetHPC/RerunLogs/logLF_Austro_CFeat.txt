lstm_units 40
epochs 20
batch_size 128
xmaxlen 12
regularization factor 0.02
dropout 0.1
LR 0.001
Embedding Size 10
Tokenize Simple False
Using Concept Fold Data False
Language Features False
Concept Features True
30  CHARACTERS
['3', '5', '7', '8', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z', '~']
100  LANGUAGES
['Teanu', 'Banjarese Malay', 'Lampung', 'Patpatar', 'Tabar', 'Tontemboan', 'Ambrym, South-East', 'Magori (South East Papua)', 'Futuna-Aniwa', 'Wuna', 'Tikopia', 'Cheke Holo', 'Windesi Wandamen', 'Gapapaiwa', 'Bunun, Southern', 'Tunjung', 'Tigak', 'Manam', 'Roti (Termanu Dialect)', 'Tetum', 'Sekar', 'Vitu', 'Alune', 'Tongan', 'Dobuan', 'Savu', 'Makassar', 'Watubela', 'Carolinian', 'Katingan', 'Soboyo', 'Kisar', 'Mambai', 'Tboli (Tagabili)', 'Sasak', 'Wogeo', 'Lenakel', 'Toambaita', 'Western Bukidnon Manobo', 'Baree', 'Molima', 'Wolio', 'Anejom (Aneityum)', 'Sengseng', 'Dehu', 'Ubir', 'Marshallese (E. Dialect)', 'Nakanai (Bileki Dialect)', 'Paiwan (Kulalao)', 'Rotuman', 'Singhi', 'Ujir (N.Aru)', 'Tsou', 'Futuna, East', 'Jawe', 'Bonfia', 'Samoan', 'Waropen', 'Santa Ana', 'Kapingamarangi', 'Kanakanabu', 'Melayu Ambon', 'Tuvalu', 'Lahanan', 'Kwaraae (Solomon Islands)', 'Maanyan', 'Roviana', 'Cebuano', 'Rejang Rejang', 'Ririo', 'Bukat', 'Teop', 'Wuvulu', 'Punan Kelai', 'Kilivila', 'Itbayaten', 'Sangir', 'Chuukese', 'Varisi', 'Seimat', 'Dayak Ngaju', 'Rurutuan', 'Tae (S.Toraja)', 'Ponapean', 'Taiof', 'Yakan', 'Vaghua', 'Raga', 'Toba Batak', 'Tahitian (Modern)', 'Elat, Kei Besar', 'Belait', 'Rennellese', 'Lio, Flores Tongah', 'Koiwai (Irian Jaya)', 'Woleai', 'As', 'Sika', 'Minangkabau', 'Selaru']
Vocab Size :  32
Building model
NO MASKING
NO MASKING
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
Input Word A (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Input Word B (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Embedding Layer (Embedding)	 (None, 12, 10)	       320
____________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDrop (None, 12, 10)	       0
____________________________________________________________________________________________________
Bidir LSTM Layer (Bidirectional) (None, 12, 80)	       16320
____________________________________________________________________________________________________
LSTM Dropout Layer (SpatialDropo (None, 12, 80)	       0
____________________________________________________________________________________________________
Attention Layer (WbwAttentionLay [(None, 12, 80), (Non 25680
____________________________________________________________________________________________________
r_a_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
r_b_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
concatenate_1 (Concatenate)	 (None, 160)	       0
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 160)	       0
____________________________________________________________________________________________________
Input Concept Feat (InputLayer)	 (None, 300)	       0
____________________________________________________________________________________________________
concatenate_2 (Concatenate)	 (None, 460)	       0
____________________________________________________________________________________________________
Hidden Layer (Dense)		 (None, 20)	       9220
____________________________________________________________________________________________________
Output Layer (Dense)		 (None, 1)	       21
====================================================================================================
Total params: 51,561.0
Trainable params: 51,561.0
Non-trainable params: 0.0
____________________________________________________________________________________________________
Model Compiled
Training New Model
Epoch 1/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.795442

Training -> Precision:	0.625724059772	 Recall:  0.618840549628	 F-Score:  0.622263268841	 AUC:  0.669516354316
Testing	 -> Precision:	0.546961325967	 Recall:  0.579493957704	 F-Score:  0.562757861924	 AUC:  0.609009304805

333626/333626 [==============================] - 385s - loss: 0.7954
Epoch 2/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.693241

Training -> Precision:	0.575479338972	 Recall:  0.766895678525	 F-Score:  0.657539975619	 AUC:  0.723288672267
Testing	 -> Precision:	0.514909756736	 Recall:  0.743391238671	 F-Score:  0.608406737753	 AUC:  0.664383689757

333626/333626 [==============================] - 378s - loss: 0.6932
Epoch 3/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.649952

Training -> Precision:	0.636055539225	 Recall:  0.763522769729	 F-Score:  0.693984586505	 AUC:  0.759421673962
Testing	 -> Precision:	0.55581127733	 Recall:  0.729607250755	 F-Score:  0.63096015676	 AUC:  0.698988166682

333626/333626 [==============================] - 378s - loss: 0.6499
Epoch 4/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.622490

Training -> Precision:	0.695960157149	 Recall:  0.728029391008	 F-Score:  0.711633663366	 AUC:  0.787669917285
Testing	 -> Precision:	0.602095111407	 Recall:  0.683723564955	 F-Score:  0.640318302387	 AUC:  0.710088219108

333626/333626 [==============================] - 377s - loss: 0.6224
Epoch 5/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.597767

Training -> Precision:	0.649906859295	 Recall:  0.814676823446	 F-Score:  0.723023289015	 AUC:  0.811188652123
Testing	 -> Precision:	0.558278791255	 Recall:  0.756986404834	 F-Score:  0.642622425262	 AUC:  0.726982623853

333626/333626 [==============================] - 378s - loss: 0.5977
Epoch 6/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.571639

Training -> Precision:	0.680934526026	 Recall:  0.81608825605		 F-Score:  0.742410438214	 AUC:  0.833898372934
Testing	 -> Precision:	0.578226281674	 Recall:  0.741125377644	 F-Score:  0.649619331347	 AUC:  0.734123729311

333626/333626 [==============================] - 377s - loss: 0.5716
Epoch 7/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.547176

Training -> Precision:	0.733680994323	 Recall:  0.807432853169	 F-Score:  0.768792181741	 AUC:  0.853110234277
Testing	 -> Precision:	0.62245567958	 Recall:  0.716012084592	 F-Score:  0.665964172813	 AUC:  0.748629223151

333626/333626 [==============================] - 377s - loss: 0.5471
Epoch 8/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.526036

Training -> Precision:	0.730359179787	 Recall:  0.843912159077	 F-Score:  0.783040352828	 AUC:  0.866702546158
Testing	 -> Precision:	0.608378588053	 Recall:  0.740370090634	 F-Score:  0.667915850439	 AUC:  0.751678728636

333626/333626 [==============================] - 377s - loss: 0.5260
Epoch 9/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.508507

Training -> Precision:	0.715704413828	 Recall:  0.863122171946	 F-Score:  0.782530967872	 AUC:  0.876581008922
Testing	 -> Precision:	0.59422876692	 Recall:  0.754342900302	 F-Score:  0.664780763791	 AUC:  0.757401905195

333626/333626 [==============================] - 377s - loss: 0.5086
Epoch 10/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.491123

Training -> Precision:	0.780084154192	 Recall:  0.835038814397	 F-Score:  0.806626566416	 AUC:  0.884669611339
Testing	 -> Precision:	0.648266344661	 Recall:  0.709592145015	 F-Score:  0.677544397368	 AUC:  0.760228632585

333626/333626 [==============================] - 377s - loss: 0.4911
Epoch 11/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.478593

Training -> Precision:	0.777771537158	 Recall:  0.862291917473	 F-Score:  0.817853855884	 AUC:  0.898818242101
Testing	 -> Precision:	0.635061728395	 Recall:  0.728474320242	 F-Score:  0.67856828775	 AUC:  0.760577752365

333626/333626 [==============================] - 378s - loss: 0.4785
Epoch 12/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.465096

Training -> Precision:	0.775671289466	 Recall:  0.873012578355	 F-Score:  0.821468330697	 AUC:  0.905622398729
Testing	 -> Precision:	0.633474576271	 Recall:  0.733950151057	 F-Score:  0.680020993702	 AUC:  0.768564831924

333626/333626 [==============================] - 377s - loss: 0.4650
Epoch 13/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.453234

Training -> Precision:	0.747591322724	 Recall:  0.899497696044	 F-Score:  0.816539561263	 AUC:  0.908256425097
Testing	 -> Precision:	0.591473207768	 Recall:  0.764916918429	 F-Score:  0.667105804858	 AUC:  0.762699600093

333626/333626 [==============================] - 377s - loss: 0.4532
Epoch 14/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.441768

Training -> Precision:	0.826337481493	 Recall:  0.857279256092	 F-Score:  0.84152404238	 AUC:  0.916276046171
Testing	 -> Precision:	0.67150898856	 Recall:  0.698262839879	 F-Score:  0.684624641303	 AUC:  0.76184371503

333626/333626 [==============================] - 377s - loss: 0.4417
Epoch 15/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.432605

Training -> Precision:	0.790345345896	 Recall:  0.894941674623	 F-Score:  0.839397656037	 AUC:  0.920460581525
Testing	 -> Precision:	0.630048465267	 Recall:  0.736404833837	 F-Score:  0.679087584886	 AUC:  0.758921156822

333626/333626 [==============================] - 377s - loss: 0.4326
Epoch 16/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.422152

Training -> Precision:	0.772066568359	 Recall:  0.912864793059	 F-Score:  0.836582906926	 AUC:  0.926705451497
Testing	 -> Precision:	0.606492718447	 Recall:  0.754909365559	 F-Score:  0.672611036339	 AUC:  0.768353439223

333626/333626 [==============================] - 377s - loss: 0.4221
Epoch 17/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.415023

Training -> Precision:	0.806984251016	 Recall:  0.900307194155	 F-Score:  0.851095141154	 AUC:  0.928766795735
Testing	 -> Precision:	0.633306255077	 Recall:  0.736027190332	 F-Score:  0.680813902716	 AUC:  0.764073862634

333626/333626 [==============================] - 377s - loss: 0.4149
Epoch 18/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.407312

Training -> Precision:	0.842152889798	 Recall:  0.888579849724	 F-Score:  0.864743667434	 AUC:  0.933877316806
Testing	 -> Precision:	0.667325664989	 Recall:  0.701095166163	 F-Score:  0.68379373849	 AUC:  0.773055463783

333626/333626 [==============================] - 379s - loss: 0.4073
Epoch 19/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.399974

Training -> Precision:	0.814230822647	 Recall:  0.912667607622	 F-Score:  0.860643664887	 AUC:  0.937624072086
Testing	 -> Precision:	0.628884237643	 Recall:  0.73753776435		 F-Score:  0.678891109759	 AUC:  0.773282659481

333626/333626 [==============================] - 377s - loss: 0.4000
Epoch 20/20
333568/333626 [============================>.] - ETA: 0ss--loss::0.391595

Training -> Precision:	0.82096588589	 Recall:  0.918095396239	 F-Score:  0.866818217449	 AUC:  0.941744726501
Testing	 -> Precision:	0.636289267016	 Recall:  0.734327794562	 F-Score:  0.681802244039	 AUC:  0.770145734985

333626/333626 [==============================] - 377s - loss: 0.3915
20768/207992[============================>.].- ETA:A0sssss

Average Precision Score 0.770145734985
Training
	     precision	  recall  f1-score   support

	  0	 0.965	   0.919     0.941    237270
	  1	 0.821	   0.918     0.867     96356

avg / total	 0.923	   0.919     0.920    333626

Testing
	     precision	  recall  f1-score   support

	  0	 0.904	   0.857     0.880     15503
	  1	 0.636	   0.734     0.682	5296

avg / total	 0.836	   0.825     0.829     20799

Testing Accuracy
0.82547237848
