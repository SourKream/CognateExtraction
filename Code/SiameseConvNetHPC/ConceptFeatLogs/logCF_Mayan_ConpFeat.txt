33  CHARACTERS
[u'3', u'5', u'7', u'8', u'C', u'N', u'S', u'T', u'X', u'a', u'c', u'b', u'e', u'd', u'g', u'f', u'i', u'h', u'k', u'm', u'l', u'o', u'n', u'q', u'p', u's', u'r', u'u', u't', u'w', u'v', u'y', u'x']
30  LANGUAGES
[u'SIPAKAPENSE', u'TZUTUJIL_SAN_JUAN_LA_LAGUNA', u'MAM_NORTHERN', u'CHORTI', u'POQOMCHI_WESTERN', u'TZELTAL_BACHAJON', u'SOUTHERN_CAKCHIQUEL_SAN_ANDRES_ITZAPA', u'MAYA_YUCATAN', u'CHONTAL_TABASCO', u'CENTRAL_QUICHE', u'EASTERN_KEKCHI_CAHABON', u'TECO_TECTITAN', u'JACALTEC', u'QANJOBAL_SANTA_EULALIA', u'LACANDON', u'ZINACANTAN_TZOTZIL', u'POCOMAM_EASTERN', u'IXIL_CHAJUL', u'CHUJ', u'CHOL_TUMBALA', u'AGUACATEC', u'MOPAN', u'MOCHO', u'ITZAJ', u'HUASTEC', u'USPANTEKO', u'ACATECO_SAN_MIGUEL_ACATAN', u'SACAPULTECO_SACAPULAS_CENTRO', u'TOJOLABAL', u'CHICOMUCELTEC']
lstm_units 40
epochs 20
batch_size 128
xmaxlen 12
regularization factor 0.02
dropout 0.1
LR 0.001
Embedding Size 10
Tokenize Simple False
Using Concept Fold Data False
Vocab Size :  36
Building model
NO MASKING
NO MASKING
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
Input Word A (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Input Word B (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Embedding Layer (Embedding)	 (None, 12, 10)	       360
____________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDrop (None, 12, 10)	       0
____________________________________________________________________________________________________
Bidir LSTM Layer (Bidirectional) (None, 12, 80)	       16320
____________________________________________________________________________________________________
LSTM Dropout Layer (SpatialDropo (None, 12, 80)	       0
____________________________________________________________________________________________________
Attention Layer (WbwAttentionLay [(None, 12, 80), (Non 25680
____________________________________________________________________________________________________
r_a_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
r_b_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
concatenate_1 (Concatenate)	 (None, 160)	       0
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 160)	       0
____________________________________________________________________________________________________
Input Concept Feat (InputLayer)	 (None, 300)	       0
____________________________________________________________________________________________________
concatenate_2 (Concatenate)	 (None, 460)	       0
____________________________________________________________________________________________________
Hidden Layer (Dense)		 (None, 20)	       9220
____________________________________________________________________________________________________
Output Layer (Dense)		 (None, 1)	       21
====================================================================================================
Total params: 51,601.0
Trainable params: 51,601.0
Non-trainable params: 0.0
____________________________________________________________________________________________________
Model Compiled
Training New Model
Epoch 1/20
28160/28222 [============================>.] - ETA: 0ss--loss::1.00942

Training -> Precision:	0.667809393212	 Recall:  0.743369585957	 F-Score:  0.703566591422
Testing	 -> Precision:	0.457646887301	 Recall:  0.730509657901	 F-Score:  0.562746504123

28222/28222 [==============================] - 48s - loss: 1.0086
Epoch 2/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.75913

Training -> Precision:	0.655595096876	 Recall:  0.790879603129	 F-Score:  0.716910969862
Testing	 -> Precision:	0.487449271006	 Recall:  0.754712590179	 F-Score:  0.592328767123

28222/28222 [==============================] - 39s - loss: 0.7588
Epoch 3/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.72577

Training -> Precision:	0.743619047619	 Recall:  0.744896012211	 F-Score:  0.744256982175
Testing	 -> Precision:	0.561922365989	 Recall:  0.636723295322	 F-Score:  0.596988871918

28222/28222 [==============================] - 39s - loss: 0.7259
Epoch 4/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.66940

Training -> Precision:	0.6881476439	 Recall:  0.867964128983	 F-Score:  0.7676665401
Testing	 -> Precision:	0.541022781584	 Recall:  0.790318827089	 F-Score:  0.642330243995

28222/28222 [==============================] - 39s - loss: 0.6695
Epoch 5/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.60713

Training -> Precision:	0.761011139674	 Recall:  0.847261972906	 F-Score:  0.801823763091
Testing	 -> Precision:	0.567938357452	 Recall:  0.703281359088	 F-Score:  0.62840507382

28222/28222 [==============================] - 39s - loss: 0.6076
Epoch 6/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.57519

Training -> Precision:	0.718896530166	 Recall:  0.877599694715	 F-Score:  0.790359996563
Testing	 -> Precision:	0.577296744503	 Recall:  0.788224342565	 F-Score:  0.666469893743

28222/28222 [==============================] - 38s - loss: 0.5750
Epoch 7/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.54927

Training -> Precision:	0.743668397043	 Recall:  0.902022514787	 F-Score:  0.815226763235
Testing	 -> Precision:	0.575234842015	 Recall:  0.783802653014	 F-Score:  0.663514578408

28222/28222 [==============================] - 38s - loss: 0.5489
Epoch 8/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.52747

Training -> Precision:	0.775904809122	 Recall:  0.895821408128	 F-Score:  0.831562167906
Testing	 -> Precision:	0.629324855059	 Recall:  0.783104491506	 F-Score:  0.697843218582

28222/28222 [==============================] - 38s - loss: 0.5277
Epoch 9/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.51459

Training -> Precision:	0.837337057728	 Recall:  0.857946956688	 F-Score:  0.847516727924
Testing	 -> Precision:	0.680816888481	 Recall:  0.690481731441	 F-Score:  0.6856152513

28222/28222 [==============================] - 38s - loss: 0.5145
Epoch 10/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.50570

Training -> Precision:	0.802120141343	 Recall:  0.887903071933	 F-Score:  0.842834503056
Testing	 -> Precision:	0.665102998164	 Recall:  0.758901559227	 F-Score:  0.708913043478

28222/28222 [==============================] - 38s - loss: 0.5059
Epoch 11/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.49275

Training -> Precision:	0.764677613136	 Recall:  0.910799465751	 F-Score:  0.831366743589
Testing	 -> Precision:	0.634563629373	 Recall:  0.768210379334	 F-Score:  0.695020528477

28222/28222 [==============================] - 38s - loss: 0.4927
Epoch 12/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.48537

Training -> Precision:	0.82312626307	 Recall:  0.893722572028	 F-Score:  0.856972968028
Testing	 -> Precision:	0.70177383592	 Recall:  0.73656039097		 F-Score:  0.718746451686

28222/28222 [==============================] - 38s - loss: 0.4854
Epoch 13/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.47393

Training -> Precision:	0.827900455661	 Recall:  0.901354703301	 F-Score:  0.86306750708
Testing	 -> Precision:	0.689360816157	 Recall:  0.770537584361	 F-Score:  0.727692307692

28222/28222 [==============================] - 38s - loss: 0.4742
Epoch 14/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.47024

Training -> Precision:	0.792913061393	 Recall:  0.917954588819	 F-Score:  0.85086439404
Testing	 -> Precision:	0.655093036639	 Recall:  0.79474051664		 F-Score:  0.718191377497

28222/28222 [==============================] - 38s - loss: 0.4702
Epoch 15/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.45957

Training -> Precision:	0.788044811514	 Recall:  0.919385613433	 F-Score:  0.848663643169
Testing	 -> Precision:	0.663073304586	 Recall:  0.787293460554	 F-Score:  0.7198638153

28222/28222 [==============================] - 38s - loss: 0.4592
Epoch 16/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.45602

Training -> Precision:	0.831885856079	 Recall:  0.895535203205	 F-Score:  0.862537903152
Testing	 -> Precision:	0.685189142979	 Recall:  0.74610193158		 F-Score:  0.714349376114

28222/28222 [==============================] - 38s - loss: 0.4561
Epoch 17/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.44542

Training -> Precision:	0.83153366693	 Recall:  0.903644342683	 F-Score:  0.866090613999
Testing	 -> Precision:	0.688040497785	 Recall:  0.75913427973		 F-Score:  0.721841115291

28222/28222 [==============================] - 38s - loss: 0.4453
Epoch 18/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.43816

Training -> Precision:	0.838989739542	 Recall:  0.912707498569	 F-Score:  0.874297464016
Testing	 -> Precision:	0.678348439074	 Recall:  0.783802653014	 F-Score:  0.727272727273

28222/28222 [==============================] - 38s - loss: 0.4384
Epoch 19/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.42993

Training -> Precision:	0.827010362251	 Recall:  0.921293646251	 F-Score:  0.871609729681
Testing	 -> Precision:	0.684115160056	 Recall:  0.790784268094	 F-Score:  0.733592400691

28222/28222 [==============================] - 38s - loss: 0.4298
Epoch 20/20
28160/28222 [============================>.] - ETA: 0ss--loss::0.42229

Training -> Precision:	0.849344016906	 Recall:  0.920244228201	 F-Score:  0.883373780851
Testing	 -> Precision:	0.711101302141	 Recall:  0.749825459623	 F-Score:  0.729950158586

28222/28222 [==============================] - 38s - loss: 0.4219
12256/12344 [============================>.] - ETA: 0ss

Average Precision Score 0.784920332855
Training
	     precision	  recall  f1-score   support

	  0	 0.950	   0.904     0.926     17740
	  1	 0.849	   0.920     0.883     10482

avg / total	 0.913	   0.910     0.910     28222

Testing
	     precision	  recall  f1-score   support

	  0	 0.862	   0.837     0.850	8047
	  1	 0.711	   0.750     0.730	4297

avg / total	 0.810	   0.807     0.808     12344

Testing Accuracy
0.806869734284
