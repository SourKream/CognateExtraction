32  CHARACTERS
['"', '3', '5', '7', '8', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z', '~']
52  LANGUAGES
['SWEDISH', 'DANISH', 'GUTNISH_LAU', 'OSSETIC_IRON', 'FRENCH', 'BIHARI', 'DUTCH', 'MARATHI', 'SORBIAN_UPPER', 'ORIYA', 'SLOVENIAN', 'MIDDLE_CORNISH', 'ANCIENT_GREEK', 'ARMENIAN_EASTERN', 'OLD_SWEDISH', 'ICELANDIC', 'SLOVAK', 'ENGLISH', 'ASSAMESE', 'BRETON', 'ITALIAN', 'ELFDALIAN', 'UKRAINIAN', 'CZECH', 'STAVANGERSK', 'NORWEGIAN_RIKSMAL', 'OLD_NORSE', 'SPANISH', 'MAGAHI', 'OLD_CHURCH_SLAVONIC', 'PORTUGUESE', 'OLD_IRISH', 'IRISH', 'MIDDLE_BRETON', 'GERMAN', 'DANISH_FJOLDE', 'OSSETIC', 'MACEDONIAN', 'LATIN', 'BELARUSIAN', 'FAROESE', 'POLISH', 'FRISIAN', 'BULGARIAN', 'GREEK', 'CLASSICAL_ARMENIAN', 'SORBIAN_LOWER', 'URDU', 'CATALAN', 'SERBO-CROATIAN', 'RUSSIAN', 'OSSETIC_DIGOR']
lstm_units 70
epochs 20
batch_size 128
xmaxlen 12
regularization factor 0.02
dropout 0.1
LR 0.001
Embedding Size 10
Tokenize Simple False
Using Concept Fold Data False
Vocab Size :  35
Building model
NO MASKING
NO MASKING
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
Input Word A (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Input Word B (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Embedding Layer (Embedding)	 (None, 12, 10)	       350
____________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDrop (None, 12, 10)	       0
____________________________________________________________________________________________________
Bidir LSTM Layer (Bidirectional) (None, 12, 140)       45360
____________________________________________________________________________________________________
LSTM Dropout Layer (SpatialDropo (None, 12, 140)       0
____________________________________________________________________________________________________
Attention Layer (WbwAttentionLay [(None, 12, 140), (No 78540
____________________________________________________________________________________________________
r_a_n (Lambda)			 (None, 140)	       0
____________________________________________________________________________________________________
r_b_n (Lambda)			 (None, 140)	       0
____________________________________________________________________________________________________
concatenate_1 (Concatenate)	 (None, 280)	       0
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 280)	       0
____________________________________________________________________________________________________
Input Concept Feat (InputLayer)	 (None, 300)	       0
____________________________________________________________________________________________________
concatenate_2 (Concatenate)	 (None, 580)	       0
____________________________________________________________________________________________________
Hidden Layer (Dense)		 (None, 20)	       11620
____________________________________________________________________________________________________
Output Layer (Dense)		 (None, 1)	       21
====================================================================================================
Total params: 135,891.0
Trainable params: 135,891.0
Non-trainable params: 0.0
____________________________________________________________________________________________________
Model Compiled
Training New Model
Epoch 1/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.734899

Training -> Precision:	0.637124242371	 Recall:  0.712755979365	 F-Score:  0.672821345237
Testing	 -> Precision:	0.680988958233	 Recall:  0.718409723981	 F-Score:  0.699199014171

204233/204233 [==============================] - 294s - loss: 0.7348
Epoch 2/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.588277

Training -> Precision:	0.626292864497	 Recall:  0.772647334688	 F-Score:  0.691814436056
Testing	 -> Precision:	0.683189200591	 Recall:  0.820207647506	 F-Score:  0.745454545455

204233/204233 [==============================] - 284s - loss: 0.5882
Epoch 3/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.548700

Training -> Precision:	0.644647601188	 Recall:  0.806061435048	 F-Score:  0.716374624455
Testing	 -> Precision:	0.704564846416	 Recall:  0.836414282097	 F-Score:  0.76484890587

204233/204233 [==============================] - 284s - loss: 0.5487
Epoch 4/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.521901

Training -> Precision:	0.680935821456	 Recall:  0.81499140222		 F-Score:  0.741957003211
Testing	 -> Precision:	0.723114680946	 Recall:  0.820714104837	 F-Score:  0.768829320365

204233/204233 [==============================] - 282s - loss: 0.5220
Epoch 5/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.495095

Training -> Precision:	0.71183506992	 Recall:  0.811649992184	 F-Score:  0.758472719305
Testing	 -> Precision:	0.747172478794	 Recall:  0.802988098253	 F-Score:  0.774075430245

204233/204233 [==============================] - 283s - loss: 0.4950
Epoch 6/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.468465

Training -> Precision:	0.737537453348	 Recall:  0.822494919494	 F-Score:  0.777702846268
Testing	 -> Precision:	0.78105211885	 Recall:  0.81210433021		 F-Score:  0.796275605214

204233/204233 [==============================] - 282s - loss: 0.4684
Epoch 7/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.444977

Training -> Precision:	0.746894516505	 Recall:  0.85063310927		 F-Score:  0.795395578293
Testing	 -> Precision:	0.763597833765	 Recall:  0.821220562168	 F-Score:  0.791361639824

204233/204233 [==============================] - 284s - loss: 0.4449
Epoch 8/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.423169

Training -> Precision:	0.742455192409	 Recall:  0.880686259184	 F-Score:  0.805684662138
Testing	 -> Precision:	0.765348837209	 Recall:  0.833375538111	 F-Score:  0.797914898776

204233/204233 [==============================] - 284s - loss: 0.4231
Epoch 9/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.401471

Training -> Precision:	0.766225941423	 Recall:  0.894599030796	 F-Score:  0.825451201702
Testing	 -> Precision:	0.764418604651	 Recall:  0.832362623449	 F-Score:  0.796945084253

204233/204233 [==============================] - 288s - loss: 0.4014
Epoch 10/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.384673

Training -> Precision:	0.760476221414	 Recall:  0.914901516336	 F-Score:  0.830571915135
Testing	 -> Precision:	0.756395743717	 Recall:  0.846036971385	 F-Score:  0.798709060483

204233/204233 [==============================] - 283s - loss: 0.3846
Epoch 11/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.365733

Training -> Precision:	0.774082776071	 Recall:  0.920607315929	 F-Score:  0.841010719482
Testing	 -> Precision:	0.758122743682	 Recall:  0.850848316029	 F-Score:  0.801813626059

204233/204233 [==============================] - 281s - loss: 0.3657
Epoch 12/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.349231

Training -> Precision:	0.816573524275	 Recall:  0.913650930123	 F-Score:  0.862388874544
Testing	 -> Precision:	0.791306436119	 Recall:  0.834388452773	 F-Score:  0.812276593122

204233/204233 [==============================] - 282s - loss: 0.3492
Epoch 13/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.334634

Training -> Precision:	0.797516064224	 Recall:  0.938564952321	 F-Score:  0.862310709765
Testing	 -> Precision:	0.766117969822	 Recall:  0.84856925804		 F-Score:  0.805238495735

204233/204233 [==============================] - 280s - loss: 0.3346
Epoch 14/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.319944

Training -> Precision:	0.837488408588	 Recall:  0.917676254494	 F-Score:  0.875750568754
Testing	 -> Precision:	0.796453546454	 Recall:  0.807546214231	 F-Score:  0.801961523953

204233/204233 [==============================] - 280s - loss: 0.3199
Epoch 15/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.307572

Training -> Precision:	0.837567520474	 Recall:  0.939268407066	 F-Score:  0.885507433266
Testing	 -> Precision:	0.794599807136	 Recall:  0.834641681438	 F-Score:  0.814128689638

204233/204233 [==============================] - 279s - loss: 0.3076
Epoch 16/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.295990

Training -> Precision:	0.832885767403	 Recall:  0.945443176489	 F-Score:  0.885602372149
Testing	 -> Precision:	0.793861768971	 Recall:  0.831856166118	 F-Score:  0.812414987016

204233/204233 [==============================] - 281s - loss: 0.2959
Epoch 17/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.286564

Training -> Precision:	0.875788629153	 Recall:  0.938525871502	 F-Score:  0.906072553717
Testing	 -> Precision:	0.825842696629	 Recall:  0.818941504178	 F-Score:  0.822377622378

204233/204233 [==============================] - 281s - loss: 0.2865
Epoch 18/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.276360

Training -> Precision:	0.869634539592	 Recall:  0.94855010161		 F-Score:  0.90737971513
Testing	 -> Precision:	0.819028441983	 Recall:  0.824006077488	 F-Score:  0.821509719768

204233/204233 [==============================] - 280s - loss: 0.2762
Epoch 19/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.265158

Training -> Precision:	0.856906825093	 Recall:  0.95263404721		 F-Score:  0.902238384736
Testing	 -> Precision:	0.789818181818	 Recall:  0.82501899215		 F-Score:  0.807034926926

204233/204233 [==============================] - 283s - loss: 0.2652
Epoch 20/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.258908

Training -> Precision:	0.872755687368	 Recall:  0.96029388776		 F-Score:  0.914434572266
Testing	 -> Precision:	0.800634455832	 Recall:  0.830843251456	 F-Score:  0.815459177333

204233/204233 [==============================] - 282s - loss: 0.2589
13184/132063[============================>.].- ETA:A0s0sss

Average Precision Score 0.88492354017
Training
	     precision	  recall  f1-score   support

	  0	 0.986	   0.953     0.969    153057
	  1	 0.873	   0.960     0.914     51176

avg / total	 0.958	   0.955     0.956    204233

Testing
	     precision	  recall  f1-score   support

	  0	 0.927	   0.912     0.919	9257
	  1	 0.801	   0.831     0.815	3949

avg / total	 0.889	   0.888     0.888     13206

Testing Accuracy
0.88755111313
