32  CHARACTERS
['"', '3', '5', '7', '8', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z', '~']
52  LANGUAGES
['SWEDISH', 'DANISH', 'GUTNISH_LAU', 'OSSETIC_IRON', 'FRENCH', 'BIHARI', 'DUTCH', 'MARATHI', 'SORBIAN_UPPER', 'ORIYA', 'SLOVENIAN', 'MIDDLE_CORNISH', 'ANCIENT_GREEK', 'ARMENIAN_EASTERN', 'OLD_SWEDISH', 'ICELANDIC', 'SLOVAK', 'ENGLISH', 'ASSAMESE', 'BRETON', 'ITALIAN', 'ELFDALIAN', 'UKRAINIAN', 'CZECH', 'STAVANGERSK', 'NORWEGIAN_RIKSMAL', 'OLD_NORSE', 'SPANISH', 'MAGAHI', 'OLD_CHURCH_SLAVONIC', 'PORTUGUESE', 'OLD_IRISH', 'IRISH', 'MIDDLE_BRETON', 'GERMAN', 'DANISH_FJOLDE', 'OSSETIC', 'MACEDONIAN', 'LATIN', 'BELARUSIAN', 'FAROESE', 'POLISH', 'FRISIAN', 'BULGARIAN', 'GREEK', 'CLASSICAL_ARMENIAN', 'SORBIAN_LOWER', 'URDU', 'CATALAN', 'SERBO-CROATIAN', 'RUSSIAN', 'OSSETIC_DIGOR']
lstm_units 40
epochs 20
batch_size 128
xmaxlen 12
regularization factor 0.02
dropout 0.1
LR 0.001
Embedding Size 10
Tokenize Simple False
Using Concept Fold Data False
Vocab Size :  35
Building model
NO MASKING
NO MASKING
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
Input Word A (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Input Word B (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Embedding Layer (Embedding)	 (None, 12, 10)	       350
____________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDrop (None, 12, 10)	       0
____________________________________________________________________________________________________
Bidir LSTM Layer (Bidirectional) (None, 12, 80)	       16320
____________________________________________________________________________________________________
LSTM Dropout Layer (SpatialDropo (None, 12, 80)	       0
____________________________________________________________________________________________________
Attention Layer (WbwAttentionLay [(None, 12, 80), (Non 25680
____________________________________________________________________________________________________
r_a_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
r_b_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
concatenate_1 (Concatenate)	 (None, 160)	       0
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 160)	       0
____________________________________________________________________________________________________
Hidden Layer (Dense)		 (None, 20)	       3220
____________________________________________________________________________________________________
Output Layer (Dense)		 (None, 1)	       21
====================================================================================================
Total params: 45,591.0
Trainable params: 45,591.0
Non-trainable params: 0.0
____________________________________________________________________________________________________
Model Compiled
Training New Model
Epoch 1/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.8074250

Training -> Precision:	0.557521591285	 Recall:  0.362025168048	 F-Score:  0.438992026728
Testing	 -> Precision:	0.600263852243	 Recall:  0.345657128387	 F-Score:  0.438695163105

204233/204233 [==============================] - 247s - loss: 0.8074
Epoch 2/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.644706

Training -> Precision:	0.674587836239	 Recall:  0.668418790058	 F-Score:  0.671489144517
Testing	 -> Precision:	0.72034820457	 Recall:  0.670549506204	 F-Score:  0.694557377049

204233/204233 [==============================] - 234s - loss: 0.6447
Epoch 3/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.590960

Training -> Precision:	0.662484224389	 Recall:  0.707753634516	 F-Score:  0.684371132462
Testing	 -> Precision:	0.717184368737	 Recall:  0.724993669283	 F-Score:  0.721067875582

204233/204233 [==============================] - 234s - loss: 0.5908
Epoch 4/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.570234

Training -> Precision:	0.640362246055	 Recall:  0.772373768954	 F-Score:  0.700200173602
Testing	 -> Precision:	0.690616500453	 Recall:  0.771587743733	 F-Score:  0.728860184188

204233/204233 [==============================] - 234s - loss: 0.5702
Epoch 5/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.551364

Training -> Precision:	0.677219366491	 Recall:  0.762017351884	 F-Score:  0.717120264803
Testing	 -> Precision:	0.714994096812	 Recall:  0.766776399088	 F-Score:  0.739980449658

204233/204233 [==============================] - 236s - loss: 0.5513
Epoch 6/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.5376090

Training -> Precision:	0.663808181494	 Recall:  0.763854150383	 F-Score:  0.710325716622
Testing	 -> Precision:	0.697743332574	 Recall:  0.775132945049	 F-Score:  0.734404990403

204233/204233 [==============================] - 236s - loss: 0.5376
Epoch 7/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.521026

Training -> Precision:	0.695928290851	 Recall:  0.785856651555	 F-Score:  0.738163630524
Testing	 -> Precision:	0.716156184241	 Recall:  0.77563940238		 F-Score:  0.744711889132

204233/204233 [==============================] - 236s - loss: 0.5209
Epoch 8/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.506239

Training -> Precision:	0.679404474647	 Recall:  0.808777551977	 F-Score:  0.738467577188
Testing	 -> Precision:	0.70500223314	 Recall:  0.799442896936	 F-Score:  0.7492583363

204233/204233 [==============================] - 234s - loss: 0.5061
Epoch 9/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.493652

Training -> Precision:	0.71965261687	 Recall:  0.795040644052	 F-Score:  0.755470556015
Testing	 -> Precision:	0.738140417457	 Recall:  0.788047606989	 F-Score:  0.762278015922

204233/204233 [==============================] - 238s - loss: 0.4936
Epoch 10/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.480936

Training -> Precision:	0.713168210061	 Recall:  0.814131624199	 F-Score:  0.760312782284
Testing	 -> Precision:	0.719168591224	 Recall:  0.78855406432		 F-Score:  0.752264766276

204233/204233 [==============================] - 234s - loss: 0.4808
Epoch 11/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.470331

Training -> Precision:	0.716209468007	 Recall:  0.840765202439	 F-Score:  0.773505195412
Testing	 -> Precision:	0.718383017164	 Recall:  0.805520384908	 F-Score:  0.759460427361

204233/204233 [==============================] - 234s - loss: 0.4703
Epoch 12/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.460493

Training -> Precision:	0.726507808293	 Recall:  0.843598561826	 F-Score:  0.78068716094
Testing	 -> Precision:	0.726021110601	 Recall:  0.801215497594	 F-Score:  0.761767184302

204233/204233 [==============================] - 234s - loss: 0.4604
Epoch 13/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.455242

Training -> Precision:	0.735143364628	 Recall:  0.849187118962	 F-Score:  0.788060675848
Testing	 -> Precision:	0.736842105263	 Recall:  0.811851101545	 F-Score:  0.772530120482

204233/204233 [==============================] - 236s - loss: 0.4552
Epoch 14/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.4425924

Training -> Precision:	0.743720725002	 Recall:  0.856319368454	 F-Score:  0.796058128974
Testing	 -> Precision:	0.740103675778	 Recall:  0.795391238288	 F-Score:  0.766752105456

204233/204233 [==============================] - 236s - loss: 0.4425
Epoch 15/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.432683

Training -> Precision:	0.771610312539	 Recall:  0.848581366265	 F-Score:  0.808267493044
Testing	 -> Precision:	0.761467889908	 Recall:  0.777665231704	 F-Score:  0.769481332999

204233/204233 [==============================] - 234s - loss: 0.4326
Epoch 16/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.422319

Training -> Precision:	0.748772779235	 Recall:  0.870329842113	 F-Score:  0.804988252304
Testing	 -> Precision:	0.741768579492	 Recall:  0.798683210939	 F-Score:  0.769174490916

204233/204233 [==============================] - 236s - loss: 0.4223
Epoch 17/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.413847

Training -> Precision:	0.749149381666	 Recall:  0.894892136939	 F-Score:  0.815560779286
Testing	 -> Precision:	0.738014087707	 Recall:  0.822486705495	 F-Score:  0.777964071856

204233/204233 [==============================] - 237s - loss: 0.4138
Epoch 18/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.4054420

Training -> Precision:	0.792717136884	 Recall:  0.866929810849	 F-Score:  0.828164229114
Testing	 -> Precision:	0.772214040255	 Recall:  0.796657381616	 F-Score:  0.784245294778

204233/204233 [==============================] - 236s - loss: 0.4054
Epoch 19/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.398201

Training -> Precision:	0.778537358153	 Recall:  0.883460997343	 F-Score:  0.827687209952
Testing	 -> Precision:	0.763645106997	 Recall:  0.80425424158		 F-Score:  0.783423778984

204233/204233 [==============================] - 235s - loss: 0.3982
Epoch 20/20
204160/204233 [============================>.] - ETA: 0ss--loss::0.3891240

Training -> Precision:	0.789013538312	 Recall:  0.890554166015	 F-Score:  0.836714461437
Testing	 -> Precision:	0.757473481196	 Recall:  0.795644466954	 F-Score:  0.776089909843

204233/204233 [==============================] - 237s - loss: 0.3891
13152/132063[============================>.].- ETA:A0s0sss

Average Precision Score 0.846716075228
Training
	     precision	  recall  f1-score   support

	  0	 0.962	   0.920     0.941    153057
	  1	 0.789	   0.891     0.837     51176

avg / total	 0.918	   0.913     0.915    204233

Testing
	     precision	  recall  f1-score   support

	  0	 0.911	   0.891     0.901	9257
	  1	 0.757	   0.796     0.776	3949

avg / total	 0.865	   0.863     0.864     13206

Testing Accuracy
0.862713917916
