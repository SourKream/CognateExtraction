160  CHARACTERS
[u'o', u'k', u's', u'y', u'\u0301', u'\u02cc', u'\u031e', u'f', u't', u'e', u'\u02c8', u'r', u'w', u'u', u'\u027e', u'\u0268', u'\u0281', u'\u010d', u'i', u'j', u'g', u'\u0259', u'\u032a', u'z', u'd', u'\u02b0', u'a', u':', u'\u0282', u'\u0273', u'\u0254', u'\u028a', u'\u0306', u'\u0283', u'\u0258', u'\u031f', u'v', u'\u0266', u'\u026a', u'\u02d0', u'\u0280', u'\u030c', u'\u0263', u'b', u'p', u'x', u'\u02b7', u'\u0325', u'\u028f', u'\u0279', u'\u028b', u'h', u'\u0251', u'\u025b', u'\u03c7', u'\u0250', u'\u0261', u'l', u'\xf0', u'\u0303', u'\u028e', u'n', u'\u02b9', u'\u0278', u'\u03b2', u'm', u'\u025f', u'\u02e0', u'\xe9', u'\u032f', u'-', u'\xe6', u'c', u'\u0294', u'\u0302', u'\u0255', u'.', u'\u02b2', u'\u0291', u'\u031d', u'\u030a', u'\u0292', u'\u0300', u'\u0289', u'\u028c', u'\u01dd', u'\xf3', u'\u0142', u'\u026d', u'\u0329', u'\xfb', u'\u0264', u'\u026b', u'\u0290', u'\u027d', u'\u02c0', u'\xe7', u'\u02a3', u'\u0252', u'_', u'\u0272', u'\xf8', u'\xe3', u'\xe1', u'\u017e', u'\u014b', u'\u0267', u'\u0275', u'\u02a7', u'\u0288', u'\u027b', u'\u0256', u'\u0320', u'\u035c', u'\u03b8', u'\u02a8', u'\u0153', u'\xee', u'\u0270', u'\xed', u'\xe2', u'\u0169', u'\u01f0', u'\u02a4', u'\u031c', u'\u1e7d', u'\u0265', u'\u012d', u'\u02d1', u'\u029d', u'\u025c', u'\u0304', u'\u026f', u'\u011b', u'\u1ebd', u'\u0361', u'\u0161', u'\u02b1', u'\u01ce', u'\xea', u'\u0276', u'\u1e59', u'\u02a6', u'\u016d', u'\u0311', u'\u0129', u'\u028d', u'\u02a5', u'q', u'\xf9', u'\xf5', u'\u01d0', u'\xf4', u'\u0324', u'\xec', u'\u01d4', u'\xfa', u'\u033b', u'\u1d58', u'?']
52  LANGUAGES
[u'ANCIENT_GREEK', u'GREEK', u'CLASSICAL_ARMENIAN', u'ARMENIAN_EASTERN', u'OSSETIC', u'OSSETIC_IRON', u'OSSETIC_DIGOR', u'BIHARI', u'URDU', u'MARATHI', u'OLD_CHURCH_SLAVONIC', u'SERBO-CROATIAN', u'BULGARIAN', u'MACEDONIAN', u'RUSSIAN', u'POLISH', u'BELARUSIAN', u'UKRAINIAN', u'SLOVAK', u'CZECH', u'SORBIAN_UPPER', u'SORBIAN_LOWER', u'SLOVENIAN', u'OLD_NORSE', u'ICELANDIC', u'FAROESE', u'NORWEGIAN_RIKSMAL', u'STAVANGERSK', u'OLD_SWEDISH', u'SWEDISH', u'ELFDALIAN', u'DANISH', u'DANISH_FJOLDE', u'GUTNISH_LAU', u'ENGLISH', u'FRISIAN', u'DUTCH', u'GERMAN', u'LATIN', u'PORTUGUESE', u'SPANISH', u'FRENCH', u'ITALIAN', u'OLD_IRISH', u'MIDDLE_CORNISH', u'MIDDLE_BRETON', u'IRISH', u'ORIYA', u'MAGAHI', u'CATALAN', u'BRETON', u'ASSAMESE']
lstm_units 40
epochs 20
batch_size 128
xmaxlen 12
regularization factor 0.02
dropout 0.1
LR 0.001
Embedding Size 45
Tokenize Simple False
Using Concept Fold Data False
No. of concepts 207
No. of training concepts 144 testing concepts 63
Vocab Size :  527
Building model
MASKING PRESENT
MASKING PRESENT
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
Input Word A (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Input Word B (InputLayer)	 (None, 12)	       0
____________________________________________________________________________________________________
Embedding Layer (Embedding)	 (None, 12, 45)	       23715
____________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDrop (None, 12, 45)	       0
____________________________________________________________________________________________________
Bidir LSTM Layer (Bidirectional) (None, 12, 80)	       27520
____________________________________________________________________________________________________
LSTM Dropout Layer (SpatialDropo (None, 12, 80)	       0
____________________________________________________________________________________________________
Attention Layer (WbwAttentionLay [(None, 12, 80), (Non 25680
____________________________________________________________________________________________________
r_a_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
r_b_n (Lambda)			 (None, 80)	       0
____________________________________________________________________________________________________
concatenate_1 (Concatenate)	 (None, 160)	       0
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 160)	       0
____________________________________________________________________________________________________
Output Layer (Dense)		 (None, 1)	       161
====================================================================================================
Total params: 77,076.0
Trainable params: 77,076.0
Non-trainable params: 0.0
____________________________________________________________________________________________________
Model Compiled
Training New Model
Epoch 1/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.715720

Training -> Precision:	0.626296845225	 Recall:  0.764575693446	 F-Score:  0.688562512275
Testing	 -> Precision:	0.454726595675	 Recall:  0.646076019864	 F-Score:  0.533770441518

223715/223715 [==============================] - 342s - loss: 0.7157
Epoch 2/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.554055

Training -> Precision:	0.693775493099	 Recall:  0.802345681007	 F-Score:  0.744121238454
Testing	 -> Precision:	0.480326748462	 Recall:  0.68496774493		 F-Score:  0.564678425221

223715/223715 [==============================] - 333s - loss: 0.5541
Epoch 3/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.490174

Training -> Precision:	0.703226065109	 Recall:  0.850083197364	 F-Score:  0.769712350708
Testing	 -> Precision:	0.485719725819	 Recall:  0.710354109621	 F-Score:  0.57694264875

223715/223715 [==============================] - 332s - loss: 0.4901
Epoch 4/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.466232

Training -> Precision:	0.737578451883	 Recall:  0.820164456148	 F-Score:  0.776682245493
Testing	 -> Precision:	0.521343845444	 Recall:  0.681301341254	 F-Score:  0.590685041746

223715/223715 [==============================] - 332s - loss: 0.4662
Epoch 5/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.493754

Training -> Precision:	0.640260337097	 Recall:  0.832749067059	 F-Score:  0.723927758897
Testing	 -> Precision:	0.442694741894	 Recall:  0.68496774493		 F-Score:  0.537805633495

223715/223715 [==============================] - 332s - loss: 0.4937
Epoch 6/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.632130

Training -> Precision:	0.579383121439	 Recall:  0.667000533109	 F-Score:  0.620112193327
Testing	 -> Precision:	0.425087738341	 Recall:  0.545273123869	 F-Score:  0.477737567601

223715/223715 [==============================] - 332s - loss: 0.6321
Epoch 7/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.750306

Training -> Precision:	0.462844970052	 Recall:  0.782733719972	 F-Score:  0.581712529415
Testing	 -> Precision:	0.339666750705	 Recall:  0.687798765489	 F-Score:  0.454754671822

223715/223715 [==============================] - 332s - loss: 0.7503
Epoch 8/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.771852

Training -> Precision:	0.501944932793	 Recall:  0.585774058577	 F-Score:  0.54062919338
Testing	 -> Precision:	0.370402082334	 Recall:  0.501926022184	 F-Score:  0.426248891516

223715/223715 [==============================] - 332s - loss: 0.7718
Epoch 9/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.784408

Training -> Precision:	0.582871066411	 Recall:  0.363257459492	 F-Score:  0.447576110431
Testing	 -> Precision:	0.449816801884	 Recall:  0.319069940131	 F-Score:  0.373326817083

223715/223715 [==============================] - 333s - loss: 0.7845
Epoch 10/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.829346

Training -> Precision:	0.478594029144	 Recall:  0.359202597696	 F-Score:  0.410391288298
Testing	 -> Precision:	0.285026682013	 Recall:  0.252842623103	 F-Score:  0.267971766558

223715/223715 [==============================] - 332s - loss: 0.8293
Epoch 11/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.840970

Training -> Precision:	0.405346502979	 Recall:  0.327506825415	 F-Score:  0.362292811509
Testing	 -> Precision:	0.272735515459	 Recall:  0.279203601429	 F-Score:  0.275931659213

223715/223715 [==============================] - 332s - loss: 0.8409
Epoch 12/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.853198

Training -> Precision:	0.40480608025	 Recall:  0.292547777903	 F-Score:  0.339641396902
Testing	 -> Precision:	0.250776076159	 Recall:  0.224950109064	 F-Score:  0.237162079511

223715/223715 [==============================] - 332s - loss: 0.8531
Epoch 13/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.839495

Training -> Precision:	0.492511119125	 Recall:  0.330947803751	 F-Score:  0.395880034011
Testing	 -> Precision:	0.327971403038	 Recall:  0.238455469439	 F-Score:  0.276140058582

223715/223715 [==============================] - 332s - loss: 0.8394
Epoch 14/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.828679

Training -> Precision:	0.503122190029	 Recall:  0.325406697792	 F-Score:  0.395204881446
Testing	 -> Precision:	0.314126525143	 Recall:  0.206710911032	 F-Score:  0.249342215753

223715/223715 [==============================] - 332s - loss: 0.8286
Epoch 15/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.823084

Training -> Precision:	0.502897904592	 Recall:  0.346230270916	 F-Score:  0.410111177022
Testing	 -> Precision:	0.319406825569	 Recall:  0.21891678656		 F-Score:  0.259782459039

223715/223715 [==============================] - 332s - loss: 0.8230
Epoch 16/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.823482

Training -> Precision:	0.538218185529	 Recall:  0.340802248752	 F-Score:  0.417341760883
Testing	 -> Precision:	0.358228633473	 Recall:  0.214368589595	 F-Score:  0.268226822682

223715/223715 [==============================] - 332s - loss: 0.8234
Epoch 17/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.819974

Training -> Precision:	0.453595474479	 Recall:  0.449491930663	 F-Score:  0.451534379513
Testing	 -> Precision:	0.316193034136	 Recall:  0.338747853529	 F-Score:  0.327082072999

223715/223715 [==============================] - 332s - loss: 0.8199
Epoch 18/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.816110

Training -> Precision:	0.453958180783	 Recall:  0.450687387926	 F-Score:  0.452316871494
Testing	 -> Precision:	0.319265320644	 Recall:  0.333178632756	 F-Score:  0.326073626598

223715/223715 [==============================] - 331s - loss: 0.8161
Epoch 19/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.819106

Training -> Precision:	0.529541530788	 Recall:  0.356763218688	 F-Score:  0.426311471454
Testing	 -> Precision:	0.365256735449	 Recall:  0.24727340233		 F-Score:  0.294902308075

223715/223715 [==============================] - 332s - loss: 0.8191
Epoch 20/20
223616/223715 [============================>.] - ETA: 0ss--loss::0.824859

Training -> Precision:	0.468541896999	 Recall:  0.294389428281	 F-Score:  0.361588983471
Testing	 -> Precision:	0.321400330674	 Recall:  0.207499883975	 F-Score:  0.252185684472

223715/223715 [==============================] - 331s - loss: 0.8247
103040/103092 [============================>.] - ETA: 0sss

Average Precision Score 0.279688654301
Training
	     precision	  recall  f1-score   support

	  0	 0.764	   0.872     0.814    161814
	  1	 0.469	   0.294     0.362     61901

avg / total	 0.682	   0.712     0.689    223715

Testing
	     precision	  recall  f1-score   support

	  0	 0.809	   0.884     0.845     81545
	  1	 0.321	   0.207     0.252     21547

avg / total	 0.707	   0.743     0.721    103092

Testing Accuracy
0.742792845226
